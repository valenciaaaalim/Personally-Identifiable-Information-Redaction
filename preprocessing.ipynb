{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb167a4",
   "metadata": {},
   "source": [
    "# Pre-process original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a251f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing file at .data/preprocessed_pii_dd.json\n",
      "New output path set to .data/preprocessed_pii_dd.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2355/2355 [00:00<00:00, 4186.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Sample Entry from Preprocessed Dataset:\n",
      "{\n",
      "  \"text\": \"Tiburce Evans, https://www.instagram.com/tiburce-evans, pin NO bLBeoRIe\\n001-691-518-9820x5621\\n\\nIntroduction - Identifying the Challenge:\\n\\nIn my role as a User Experience Designer at a technology startup in San Francisco, I encountered a complex challenge that required a thoughtful and innovative solution. Our team was tasked with redesigning the user experience of our mobile application to better meet the needs of a diverse user group, spanning various age ranges, cultures, ethnicities, and abilities. This challenge was significant due to the wide array of user needs and preferences to consider, as well as the potential impact on overall customer satisfaction and conversion rates.\\n\\nSelection of the Tool or Approach:\\n\\nTo address this complex challenge, I chose to apply the human-centered design (HCD) approach, a methodology that emphasizes empathy, collaboration, and iteration throughout the design process. I selected this approach due to its relevance to the challenge, as HCD focuses on understanding and meeting the needs of diverse user groups, and its theoretical underpinnings in user-centered research and design. Additionally, HCD has a proven effectiveness in similar situations, as it allows designers to create solutions that are both user-friendly and business-oriented.\\n\\nWhile there were several alternatives considered, such as design thinking, user-centered design, and participatory design, I ultimately discarded them in favor of HCD due to its comprehensive nature and emphasis on inclusivity. Design thinking, for example, focuses predominantly on the ideation and prototyping stages, while user-centered design lacks the collaborative aspect that I believed was crucial for this particular challenge. Participatory design, while inclusive, can be time-consuming and challenging to implement in a fast-paced startup environment.\\n\\nApplication of the Tool or Approach:\\n\\nTo apply the HCD approach, I followed a series of steps and processes to understand, ideate, and iterate on the mobile application's user experience. First, I conducted user research studies to empathize with our diverse user group, using various methods such as interviews, observations, and surveys. This research allowed me to identify user needs, preferences, and pain points, which informed the subsequent ideation phase.\\n\\nDuring the ideation phase, I facilitated design thinking workshops with cross-functional stakeholders, encouraging collaboration and creativity. We generated numerous ideas, selected the most promising ones, and created wireframes and prototypes to visualize and test the potential solutions. Throughout this phase, I ensured that all ideas were evaluated through the lens of inclusivity, striving to create a design that catered to users of all ages, cultures, ethnicities, mobility, literacy skills, and disabilities.\\n\\nAdaptations and customizations were made to the HCD approach to suit the specific context of the challenge. For instance, I incorporated user analytics tools like FullStory and Google Analytics to gain a better understanding of user behavior and preferences within the application. This data-driven approach complemented the empathetic research methods used in the initial stages of the HCD process.\\n\\nAnalysis and Insight:\\n\\nThe application of the HCD approach was effective in addressing the challenge of redesigning the mobile application's user experience for a diverse user group. The user research studies provided valuable insights into user needs and preferences, which guided the ideation and prototyping stages. By fostering collaboration and creativity within the design thinking workshops, we were able to generate innovative solutions that catered to a wide array of user needs.\\n\\nHowever, there were some limitations in the HCD approach that became apparent during the application process. First, the emphasis on empathy and collaboration, while valuable, can sometimes lead to prolonged decision-making and slower progress. Second, the inclusivity criteria, while essential, can be challenging to implement consistently across all stages of the process.\\n\\nNonetheless, the use of the HCD approach significantly influenced the outcome of the challenge by promoting a user-centered, collaborative, and iterative approach to design. The resulting redesign was well-received by users, with an increase in overall customer satisfaction and conversion rates. The user analytics tools provided additional data to support the effectiveness of the redesign, demonstrating increased user engagement and reduced drop-off rates.\\n\\nConclusion - Future Applications:\\n\\nThe key insights gained from applying the HCD approach to redesign the mobile application's user experience can inform future applications of the same or similar tools in addressing other challenges. First, user research studies are crucial in understanding user needs and preferences, and should be integrated into the design process from the outset. Second, fostering collaboration and creativity through design thinking workshops can lead to innovative solutions that cater to a wide array of user needs. Lastly, consistent application of inclusivity criteria throughout the design process is essential to creating solutions that are accessible and user-friendly for diverse user groups.\\n\\nIn future applications of the HCD approach or similar tools, I would propose a few modifications or improvements to enhance its effectiveness. First, implementing a more efficient decision-making process during the empathy and collaboration stages could accelerate progress without compromising user-centeredness. Second, incorporating additional data-driven methods, such as A/B testing and usability testing, alongside empathetic research methods could further inform the ideation and prototyping stages.\\n\\nIn summary, the HCD approach proved to be an effective tool in addressing the challenge of redesigning the mobile application's user experience for a diverse user group. By understanding user needs, fostering collaboration and creativity, and consistently applying inclusivity criteria, our team was able to create a solution that increased overall customer satisfaction and conversion rates. The insights gained from this experience can inform future applications of HCD, as well as other design methodologies, in addressing complex challenges within user experience design.\\n\\nThank you for considering my perspective,\\nTiburce Evans\",\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"start\": 0,\n",
      "      \"end\": 13,\n",
      "      \"label\": \"name\"\n",
      "    },\n",
      "    {\n",
      "      \"start\": 15,\n",
      "      \"end\": 54,\n",
      "      \"label\": \"url\"\n",
      "    },\n",
      "    {\n",
      "      \"start\": 63,\n",
      "      \"end\": 71,\n",
      "      \"label\": \"id number\"\n",
      "    },\n",
      "    {\n",
      "      \"start\": 72,\n",
      "      \"end\": 93,\n",
      "      \"label\": \"phone number\"\n",
      "    },\n",
      "    {\n",
      "      \"start\": 6432,\n",
      "      \"end\": 6445,\n",
      "      \"label\": \"name\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Label map from PII-DD to GLiNER-compatible\n",
    "label_map = {\n",
    "    \"NAME_STUDENT\": \"name\",\n",
    "    \"EMAIL\": \"email\",\n",
    "    \"USERNAME\": \"username\",\n",
    "    \"ID_NUM\": \"id number\",\n",
    "    \"PHONE_NUM\": \"phone number\",\n",
    "    \"URL_PERSONAL\": \"url\",\n",
    "    \"STREET_ADDRESS\": \"street address\"\n",
    "}\n",
    "\n",
    "# Load JSON data\n",
    "input_path = os.path.join(\".data/\", \"mixtral-8x7b-v1.json\")  # Update if needed\n",
    "output_path = os.path.join(\".data/\", \"preprocessed_pii_dd.json\")\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    print(f\"Removed existing file at {output_path}\")\n",
    "    output_path = os.path.join(\".data/\", \"preprocessed_pii_dd.json\")\n",
    "    print(f\"New output path set to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Helper to reconstruct full text from tokens and spacing\n",
    "def reconstruct_text(tokens, whitespaces):\n",
    "    return \"\".join([t + (\" \" if w else \"\") for t, w in zip(tokens, whitespaces)])\n",
    "\n",
    "# Extract labeled spans in GLiNER format\n",
    "def extract_labeled_spans(tokens, whitespaces, labels):\n",
    "    spans = []\n",
    "    current = None\n",
    "    start_char = 0\n",
    "\n",
    "    for token, label, has_space in zip(tokens, labels, whitespaces):\n",
    "        end_char = start_char + len(token)\n",
    "\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current:\n",
    "                spans.append(current)\n",
    "            mapped = label_map.get(label[2:])\n",
    "            if mapped:\n",
    "                current = {\"start\": start_char, \"end\": end_char, \"label\": mapped}\n",
    "            else:\n",
    "                current = None\n",
    "\n",
    "        elif label.startswith(\"I-\") and current:\n",
    "            current[\"end\"] = end_char\n",
    "\n",
    "        elif current:\n",
    "            spans.append(current)\n",
    "            current = None\n",
    "\n",
    "        start_char = end_char + (1 if has_space else 0)\n",
    "\n",
    "    if current:\n",
    "        spans.append(current)\n",
    "\n",
    "    return spans\n",
    "\n",
    "# Process and convert\n",
    "converted_data = []\n",
    "for entry in tqdm(data):\n",
    "    text = reconstruct_text(entry[\"tokens\"], entry[\"trailing_whitespace\"])\n",
    "    entities = extract_labeled_spans(entry[\"tokens\"], entry[\"trailing_whitespace\"], entry[\"labels\"])\n",
    "    converted_data.append({\"text\": text, \"entities\": entities})\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(converted_data, f, indent=2)\n",
    "\n",
    "# Print example\n",
    "print(\"\\nðŸ”Ž Sample Entry from Preprocessed Dataset:\")\n",
    "print(json.dumps(converted_data[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5885f12",
   "metadata": {},
   "source": [
    "# Augmented data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00c9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "802aace7",
   "metadata": {},
   "source": [
    "# Combination of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(\"data/preprocessed_pii_dd.json\") as f:\n",
    "#     real_data = json.load(f)\n",
    "\n",
    "# with open(\"data/augmented_name_url_id.json\") as f:\n",
    "#     augmented_data = json.load(f)\n",
    "\n",
    "# combined = real_data + augmented_data\n",
    "\n",
    "# with open(\"data/gilner_data_set.json\", \"w\") as f:\n",
    "#     json.dump(combined, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844c0f2",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1261b85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing file at .data/train_split.json\n",
      "Wrote 1884 records to .data/train_split.json\n",
      "Removed existing file at .data/val_split.json\n",
      "Wrote 235 records to .data/val_split.json\n",
      "Removed existing file at .data/test_split.json\n",
      "Wrote 236 records to .data/test_split.json\n",
      "âœ… Split complete: 1884 train / 235 val / 236 test entries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "preprocessed_path = os.path.join('.data', 'preprocessed_pii_dd.json')\n",
    "\n",
    "with open(preprocessed_path, 'r') as f:\n",
    "    preprocessed_data = json.load(f)\n",
    "\n",
    "def write_split(pathname, payload):\n",
    "    if os.path.exists(pathname):\n",
    "        os.remove(pathname)\n",
    "        print(f'Removed existing file at {pathname}')\n",
    "    os.makedirs(os.path.dirname(pathname), exist_ok=True)\n",
    "    with open(pathname, 'w') as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "    print(f'Wrote {len(payload)} records to {pathname}')\n",
    "\n",
    "train_data, temp_data = train_test_split(preprocessed_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data_path = os.path.join('.data', 'train_split.json')\n",
    "val_data_path = os.path.join('.data', 'val_split.json')\n",
    "test_data_path = os.path.join('.data', 'test_split.json')\n",
    "\n",
    "write_split(train_data_path, train_data)\n",
    "write_split(val_data_path, val_data)\n",
    "write_split(test_data_path, test_data)\n",
    "\n",
    "print(f'âœ… Split complete: {len(train_data)} train / {len(val_data)} val / {len(test_data)} test entries')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7896fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding, DataCollator\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e7bd6",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "727bd11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f525bd7f45f646d9a359ff26d51aa9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  \n",
    "\n",
    "model = GLiNER.from_pretrained(\"knowledgator/gliner-pii-base-v1.0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"knowledgator/gliner-pii-base-v1.0\")\n",
    "\n",
    "# use it for better performance, it mimics original implementation but it's less memory efficient\n",
    "data_collator = DataCollator(model.config, data_processor=model.data_processor, prepare_labels=True)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "PII_LABELS = [\n",
    "    \"name\", \"email\", \"username\", \"id number\", \"phone number\", \"url\", \"street address\"\n",
    "]\n",
    "\n",
    "# Mapping competition labels to GLiNER-compatible labels\n",
    "label_map = {\n",
    "    \"NAME_STUDENT\": \"name\",\n",
    "    \"EMAIL\": \"email\",\n",
    "    \"USERNAME\": \"username\",\n",
    "    \"ID_NUM\": \"id number\",\n",
    "    \"PHONE_NUM\": \"phone number\",\n",
    "    \"URL_PERSONAL\": \"url\",\n",
    "    \"STREET_ADDRESS\": \"street address\",\n",
    "}\n",
    "\n",
    "all_labels = PII_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50a47b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/236 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tokenized_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Run training loop\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m    112\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.models/gliner_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[33], line 71\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     72\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m data_collator(batch)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/gliner/data_processing/collator.py:29\u001b[0m, in \u001b[0;36mDataCollator.__call__\u001b[0;34m(self, input_x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_x):\n\u001b[0;32m---> 29\u001b[0m     raw_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_raw_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_processor\u001b[38;5;241m.\u001b[39mcollate_fn(raw_batch, prepare_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_labels)\n\u001b[1;32m     32\u001b[0m     model_input\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan_idx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_batch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     33\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_batch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_length\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/gliner/data_processing/processor.py:248\u001b[0m, in \u001b[0;36mBaseProcessor.collate_raw_batch\u001b[0;34m(self, batch_list, entity_types, negatives, class_to_ids, id_to_classes)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m class_to_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Generate mappings dynamically based on batch content\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     class_to_ids, id_to_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_generate_class_mappings(batch_list, negatives)\n\u001b[0;32m--> 248\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_example(b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m], class_to_ids[i]) \n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_list)\n\u001b[1;32m    251\u001b[0m     ]\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m class_to_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# Handle cases for entity_types being a list of strings or list of lists\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/gliner/data_processing/processor.py:249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m class_to_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Generate mappings dynamically based on batch content\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     class_to_ids, id_to_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_generate_class_mappings(batch_list, negatives)\n\u001b[1;32m    248\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_example(\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenized_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m], class_to_ids[i]) \n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_list)\n\u001b[1;32m    251\u001b[0m     ]\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m class_to_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# Handle cases for entity_types being a list of strings or list of lists\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokenized_text'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from gliner import GLiNER\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_steps = 500\n",
    "batch_size = 8\n",
    "data_size = len(train_data)\n",
    "num_batches = data_size // batch_size\n",
    "num_epochs = max(1, num_steps // num_batches)\n",
    "\n",
    "# Custom dataset wrapper to be used with GLiNER\n",
    "class GLiNERPyTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        return {\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"ner\": [(e[\"start\"], e[\"end\"], e[\"label\"]) for e in entry.get(\"entities\", [])],\n",
    "            \"labels\": self.labels\n",
    "        }\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = GLiNERPyTorchDataset(train_data, all_labels)\n",
    "val_dataset = GLiNERPyTorchDataset(val_data, all_labels)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# train_dataset = GLiNERDataset(train_data, label_list=all_labels, tokenizer=tokenizer)\n",
    "# val_dataset = GLiNERDataset(val_data, label_list=all_labels, tokenizer=tokenizer)\n",
    "\n",
    "data_collator = DataCollator(\n",
    "    model.config,\n",
    "    data_processor=model.data_processor,\n",
    "    prepare_labels=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_loader, val_loader, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nðŸš€ Epoch {epoch+1}/{num_epochs}\")\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs = data_collator(batch)\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"\\nðŸ“‰ Average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "        evaluate(model, val_loader)\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = data_collator(batch)\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            total_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    print(f\"âœ… Validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Run training loop\n",
    "train(model, train_loader, val_loader, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Save the model\n",
    "os.makedirs(\".models/gliner_finetuned\", exist_ok=True)\n",
    "model.save_pretrained(\".models/gliner_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5925278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GLiNER.from_pretrained(\"knowledgator/gliner-pii-base-v1.0\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"knowledgator/gliner-pii-base-v1.0\")\n",
    "\n",
    "# Assuming train_data is a list of dicts with \"text\" and \"entities\"\n",
    "all_labels = list({ent[\"label\"] for example in train_data for ent in example[\"entities\"]})\n",
    "model.set_labels(all_labels)\n",
    "\n",
    "# Create PyTorch-compatible dataset\n",
    "from gliner.data import GLiNERDataset, DataCollator\n",
    "\n",
    "train_dataset = GLiNERDataset(train_data, label_list=all_labels, tokenizer=tokenizer)\n",
    "val_dataset = GLiNERDataset(val_data, label_list=all_labels, tokenizer=tokenizer)\n",
    "\n",
    "data_collator = DataCollator(\n",
    "    model.config,\n",
    "    data_processor=model.data_processor,\n",
    "    prepare_labels=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in loop:\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"ðŸ“‰ Epoch {epoch+1} avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "# âœ… Save the fine-tuned model\n",
    "model.save_pretrained(\"./models/gliner-finetuned-adamw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69228698",
   "metadata": {},
   "source": [
    "# Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12732dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3tf4d74975j0pjkbm28_ct3c0000gn/T/ipykernel_55318/681553020.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 37\u001b[0m\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     28\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# ðŸš€ Start training\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# âœ… Save your model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.models/gliner_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/transformers/trainer.py:2514\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2512\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2513\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2514\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2516\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/transformers/trainer.py:5243\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5243\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   5244\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5245\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/accelerate/data_loader.py:567\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/gliner/data_processing/collator.py:29\u001b[0m, in \u001b[0;36mDataCollator.__call__\u001b[0;34m(self, input_x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_x):\n\u001b[0;32m---> 29\u001b[0m     raw_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_raw_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_processor\u001b[38;5;241m.\u001b[39mcollate_fn(raw_batch, prepare_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_labels)\n\u001b[1;32m     32\u001b[0m     model_input\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan_idx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_batch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     33\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_batch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_length\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/gliner/data_processing/processor.py:247\u001b[0m, in \u001b[0;36mBaseProcessor.collate_raw_batch\u001b[0;34m(self, batch_list, entity_types, negatives, class_to_ids, id_to_classes)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_raw_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_list: List[Dict], entity_types: List[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    244\u001b[0m                     negatives: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, class_to_ids: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, id_to_classes: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entity_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m class_to_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# Generate mappings dynamically based on batch content\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m         class_to_ids, id_to_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate_class_mappings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegatives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_example(b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m], class_to_ids[i]) \n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_list)\n\u001b[1;32m    251\u001b[0m         ]\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/gliner/data_processing/processor.py:217\u001b[0m, in \u001b[0;36mBaseProcessor.batch_generate_class_mappings\u001b[0;34m(self, batch_list, negatives)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbatch_generate_class_mappings\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_list: List[Dict], negatives: List[\u001b[38;5;28mstr\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[1;32m    215\u001b[0m     List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]], List[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m negatives \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m         negatives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_negatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     class_to_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    219\u001b[0m     id_to_classes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ner/lib/python3.10/site-packages/gliner/data_processing/processor.py:78\u001b[0m, in \u001b[0;36mBaseProcessor.get_negatives\u001b[0;34m(batch_list, sampled_neg)\u001b[0m\n\u001b[1;32m     76\u001b[0m ent_types \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch_list:\n\u001b[0;32m---> 78\u001b[0m     types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([el[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[1;32m     79\u001b[0m     ent_types\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(types))\n\u001b[1;32m     80\u001b[0m ent_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(ent_types))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ner'"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps = 500\n",
    "batch_size = 8\n",
    "data_size = len(train_data)\n",
    "num_batches = data_size // batch_size\n",
    "num_epochs = max(1, num_steps // num_batches)\n",
    "\n",
    "# Define training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\".models/gliner_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # disables wandb/huggingface logging\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=model.data_processor.transformer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "\n",
    ")\n",
    "\n",
    "# ðŸš€ Start training\n",
    "trainer.train()\n",
    "\n",
    "# âœ… Save your model\n",
    "model.save_pretrained(\".models/gliner_finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
